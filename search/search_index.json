{"config":{"lang":["en"],"separator":"[\\s\\-,:!=\\[\\]()\"`/]+|\\.(?!\\d)|&[lg]t;|(?!\\b)(?=[A-Z][a-z])","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to Modelando 2025 Edition","text":""},{"location":"#a-quick-introduction-en","title":"A quick Introduction - EN","text":"<p>Modelando, meaning \"modeling\" in Spanish. A bilingual repository (English, Spanish) to collaborate on important concepts related to data modeling.  I will start with some thoughts and hopefully get some interest and contributions that can influence where to go.</p> <p>My initial thought for this initiative was approximately five years ago. </p> <p>A customer asked me to prepare training for Power BI for a group of controllers (accountants) that had started to use Power BI with great results, but now they wanted to take it to the next level. </p> <p>They gave me good feedback on how I presented Dimension and Facts. </p> <p>Now I decided to remember how I did it and make it available to everyone, hoping it helps more people and sparks collaboration.  </p>"},{"location":"#una-introduccion-breve-es","title":"Una introducci\u00f3n breve - ES","text":"<p>Un repositorio biling\u00fce (ingl\u00e9s, espa\u00f1ol) para colaborar en conceptos importantes relacionados con el modelado de datos. Comenzar\u00e9 con algunos pensamientos y espero obtener alg\u00fan inter\u00e9s y contribuciones que puedan influir a d\u00f3nde ir.</p> <p>Mi pensamiento inicial para esta iniciativa fue hace aproximadamente cinco a\u00f1os.</p> <p>Un cliente me pidi\u00f3 que preparara una capacitaci\u00f3n sobre Power BI para un grupo de controladores (contadores) que hab\u00edan comenzado a usar Power BI con excelentes resultados, pero ahora quer\u00edan llevarlo al siguiente nivel.</p> <p>Me dieron buenos comentarios sobre c\u00f3mo present\u00e9 \"Dimensions\" y \"Facts\".</p> <p>Ahora decid\u00ed recordar c\u00f3mo lo hice y ponerlo a disposici\u00f3n de todos, con la esperanza de que ayude a m\u00e1s personas y genere colaboraci\u00f3n.</p>"},{"location":"Season%201/S1E1/S1E1%20One%20possible%20classification%20of%20data/","title":"Introduction","text":"<p>Data modeling is crucial in computer science, as it helps us effectively store, retrieve, and analyze data. At its core, data modeling creates a data structure blueprint. This blueprint, also known as a data model, allows us to communicate the design and organization of our data and the relationships between different data entities.  Whether you're a seasoned professional or just starting your data modeling journey, this introduction will provide you with a solid foundation and inspire you to delve deeper into this exciting field. So, let's get started!</p>"},{"location":"Season%201/S1E1/S1E1%20One%20possible%20classification%20of%20data/#data-and-reality","title":"Data and Reality","text":"<p>I like to make the first distinction of data based on what they represent in the real world. Let's start with physical objects, for example, items (in a store), customers, vendors, etc. All these objects exist in reality; you can, at some point, see them or even touch them. These objects have attributes intrinsic to themselves. For example, a customer could have characteristics like Date of Birth, Name, Last Name, etc. The other types of objects are more abstract concerning what they represent; they are usually bound to a specific time and involve one or more other objects. (or entities)</p> <p>For example, we can model an invoice, another type of object, or an entity. The invoice describes a fact (on a date), an event that involves two other objects: Customer (John doe) and Item (The coffee). Notice that the invoice without Customer and Item does not make much sense. It requires a date, a customer, an item, an amount, and many other possibles thing to make sense. In other words, to describe an invoice, you need something like: On 4th January at 7:30, John Doe bought three large coffees for 9 dollars. (Date, Customer, Quantity, Total)</p>"},{"location":"Season%201/S1E1/S1E1%20One%20possible%20classification%20of%20data/#dimensions-or-master-data","title":"Dimensions (or master data)","text":"<p>Objects that you can enumerate and exist by themselves. Usually, they represent a physical object (customer, item, etc.) but could be something abstract like a cost center, profit center, etc. </p> <p>These objects can have two primary types of attributes: non-time-dependant attributes (for example, the date of birth of a person) and time-dependent attributes (for instance, the last name of a person that can change for whatever reason during his lifetime).</p> <p>These time-dependent attributes need to have a validity interval, for example: From 03.12.1984, The name of the Person is John, until 03.12.2004 From 04.12.2004 until the end of time, the person's name is Juan.</p>"},{"location":"Season%201/S1E1/S1E1%20One%20possible%20classification%20of%20data/#facts-or-transactional-data","title":"Facts (or Transactional data)","text":"<p>They represent an event on a specific date involving one or more dimensions. In transactional data, a new type of column is available: measures or key figures.  Let's go back to the invoice example; the measures are Quantity = three, Amount = 9 dollars. These numeric columns are usually the ones you aggregate (sum) to see some totals. For example, sum the amount column for all invoices in one month to see how much you sold during that month.</p>"},{"location":"Season%201/S1E1/S1E1%20One%20possible%20classification%20of%20data/#excercise","title":"Excercise","text":"<p>I suggest you to write down at least ten dimensions and some of their attribtues and 3 Facts, to help you digest this content. Feel free to open issues and ask questions if you have any doubts. </p>"},{"location":"Season%201/S1E1/S1E1%20Una%20posible%20clasificaci%C3%B3n%20de%20los%20datos/","title":"Introducci\u00f3n","text":"<p>El modelado de datos es crucial en inform\u00e1tica, ya que nos ayuda a almacenar, recuperar y analizar datos de manera efectiva. En esencia, el modelado de datos crea un modelo de estructura de datos. Este modelo, tambi\u00e9n conocido como modelo de datos, nos permite comunicar el dise\u00f1o y la organizaci\u00f3n de nuestros datos y las relaciones entre diferentes entidades de datos. Tanto si es un profesional experimentado como si acaba de empezar su viaje de modelado de datos, esta introducci\u00f3n le proporcionar\u00e1 una base s\u00f3lida y le inspirar\u00e1 para profundizar en este apasionante campo. \u00a1Entonces empecemos!</p>"},{"location":"Season%201/S1E1/S1E1%20Una%20posible%20clasificaci%C3%B3n%20de%20los%20datos/#datos-y-realidad","title":"Datos y Realidad","text":"<p>Me gusta hacer la primera distinci\u00f3n de datos en funci\u00f3n de lo que representan en el mundo real. Comencemos con los objetos f\u00edsicos, por ejemplo, art\u00edculos (en una tienda), clientes, proveedores, etc. Todos estos objetos existen en la realidad; puedes, en alg\u00fan momento, verlos o incluso tocarlos. Estos objetos tienen atributos intr\u00ednsecos a ellos mismos. Por ejemplo, un cliente podr\u00eda tener caracter\u00edsticas como Datos de Nacimiento, Nombre y Apellido, etc. Me gusta hacer la primera distinci\u00f3n de datos en funci\u00f3n de lo que representan en el mundo real. Todos estos objetos existen en la realidad; puedes, en alg\u00fan momento, verlos o incluso tocarlos. Estos objetos tienen atributos intr\u00ednsecos a ellos mismos. Por ejemplo, un cliente podr\u00eda tener caracter\u00edsticas como fecha de nacimiento, nombre y apellido, etc. Los otros tipos de objetos son m\u00e1s abstractos en cuanto a lo que representan; generalmente est\u00e1n vinculados a un tiempo espec\u00edfico e involucran uno o m\u00e1s objetos. (o entidades)</p> <p>Por ejemplo, podemos modelar una factura. La factura describe un hecho (en una fecha), un evento que involucra a otros dos objetos: Cliente (Juan P\u00e9rez) y Art\u00edculo (El caf\u00e9). Note que la factura sin Cliente y Art\u00edculo no tiene mucho sentido. Requiere una fecha, un cliente, un art\u00edculo, una cantidad y muchas otras cosas para que tenga sentido. En otras palabras, para describir una factura, se necesita algo como: El 4 de enero a las 7:30, Juan P\u00e9rez compr\u00f3 tres caf\u00e9s grandes por 9 pesos. (Fecha, Cliente, Cantidad, Total)</p>"},{"location":"Season%201/S1E1/S1E1%20Una%20posible%20clasificaci%C3%B3n%20de%20los%20datos/#dimensiones-o-datos-maestros","title":"Dimensiones (o datos maestros)","text":"<p>Objetos que puedes enumerar y existen por s\u00ed mismos. Por lo general, representan un objeto f\u00edsico (cliente, art\u00edculo, etc.) pero pueden tambi\u00e9n ser algo abstracto como un centro de costos, un centro de ganancias, etc.</p> <p>Estos objetos pueden tener dos tipos principales de atributos: atributos no dependientes del tiempo (por ejemplo, la fecha de nacimiento de una persona) y atributos dependientes del tiempo (por ejemplo, el apellido de una persona que puede cambiar por cualquier motivo durante su vida).</p> <p>Estos atributos dependientes del tiempo deben tener un intervalo de validez, por ejemplo: Desde el 12.03.1984, El nombre de la Persona es Hugo, hasta el 12.03.2004 Desde el 12.04.2004 hasta el final de los tiempos, la persona se llama Juan.</p>"},{"location":"Season%201/S1E1/S1E1%20Una%20posible%20clasificaci%C3%B3n%20de%20los%20datos/#hechos-o-datos-transaccionales","title":"Hechos (o datos transaccionales)","text":"<p>Representan un evento en una fecha espec\u00edfica que involucra una o m\u00e1s dimensiones. En datos transaccionales, est\u00e1 disponible un nuevo tipo de columna: indicadores o cifras clave. Volvamos al ejemplo de la factura; las medidas son Cantidad = tres, Importe = 9 pesos. Estas columnas num\u00e9ricas suelen ser las que agregas (suma) para ver algunos totales. Por ejemplo, sum\u00e1 la columna de cantidad de todas las facturas en un mes para ver cu\u00e1nto vendi\u00f3 durante ese mes.</p>"},{"location":"Season%201/S1E1/S1E1%20Una%20posible%20clasificaci%C3%B3n%20de%20los%20datos/#ejercicio","title":"Ejercicio","text":"<p>Te sugiero que escribas al menos diez dimensiones y algunos de sus atributos y 3 hechos, para ayudarte a digerir este contenido. Si\u00e9ntete libre de abrir problemas y hacer preguntas si tiene alguna duda usando la funcionalidad de GitHub.</p>"},{"location":"Season%201/S1E2/S1E2-Tipos%20de%20sistemas/","title":"Tipo de Sistemas","text":"<p>Los sistemas inform\u00e1ticos de informaci\u00f3n se puede dividir en categor\u00edas principales (al menos para el prop\u00f3sito de este blog): sistemas transaccionales y anal\u00edticos/de informes.</p> <p>Los sistemas transaccionales o sistemas de registros son donde se registran los hechos del negocio: Un cliente compra art\u00edculos. A un paciente se le ha recetado medicaci\u00f3n. Se cambia el aceite de una m\u00e1quina.</p> <p>El enfoque de estos sistemas suele ser la consistencia y la concurrencia. No es bueno para el negocio perder o tener datos incompletos. Y muchas transacciones pueden ocurrir u ocurrir\u00e1n simult\u00e1neamente, lo que significa que la concurrencia ser\u00e1 alta.</p> <p>Los sistemas anal\u00edticos / de informes son donde los datos registrados por los sistemas de registro se transfieren para ser analizados. El enfoque de este sistema es mucho almacenamiento (guardar el historial de m\u00faltiples sistemas de registros) y una alta capacidad para procesar datos. Los datos pueden provenir de varios sistemas y los informes obtenidos de esos sistemas pueden ser complejos.</p>"},{"location":"Season%201/S1E2/S1E2-Tipos%20de%20sistemas/#sistemas-transaccionales-sistemas-de-registros-oltp","title":"Sistemas transaccionales, Sistemas de registros (OLTP)","text":"<p>OLTP significa Procesamiento transaccional en l\u00ednea. OLTP es una denominaci\u00f3n tambi\u00e9n dada a la base de datos detr\u00e1s de los sistemas de registros. Una base de datos es una tecnolog\u00eda que almacena datos de un sistema o aplicaci\u00f3n. Puede usar sistemas de registros, sistemas transaccionales u OLTP casi como sin\u00f3nimos. Recuerde que OLTP puede usarse m\u00e1s para indicar una base de datos o tecnolog\u00eda que un sistema. Ejemplos de sistemas transaccionales son SAP S/4 HANA, Microsoft Dynamics, un sistema bancario, un sistema de punto de venta, etc.</p>"},{"location":"Season%201/S1E2/S1E2-Tipos%20de%20sistemas/#sistemas-analiticosde-informes-olap","title":"Sistemas anal\u00edticos/de informes (OLAP)","text":"<p>OLAP significa Procesamiento anal\u00edtico en l\u00ednea. OLAP es una denominaci\u00f3n que tambi\u00e9n se le da a la base de datos detr\u00e1s de Sistemas anal\u00edticos/de informes. Ejemplos de sistemas OLAP son Azure Synapse Dedicate Pools, conjuntos de datos de Power BI (anteriormente Ms. Analysis Services), Snowflake, Google Cloud Big Query, AWS redshift, DuckDB, etc.</p>"},{"location":"Season%201/S1E2/S1E2-Tipos%20de%20sistemas/#el-mundo-no-es-0s-o-1s","title":"El mundo no es 0s o 1s","text":"<p>Habiendo terminado el bachillerato y matriculado en la universidad (Ingenier\u00eda de Software en la UPSA), me di cuenta de que la vida es m\u00e1s complicada que s\u00ed o no, 0s y 1s. La vida sucede en el medio, lo mismo sucedi\u00f3 con OLTP y OLAP; al principio, era una distinci\u00f3n clara, pero \u00bfadivinen qu\u00e9? Alrededor de 2014 apareci\u00f3 el procesamiento h\u00edbrido de transacciones/anal\u00edtico (HTAP). Era un nuevo enfoque, y su concepto central era evitar mover los datos del sistema de registros al sistema Anal\u00edtico, en otras palabras: Una sola base de datos que pudiera servir a ambos mundos, OLAP y OLTP.</p>"},{"location":"Season%201/S1E2/S1E2-Tipos%20de%20sistemas/#antes-de-htap","title":"Antes de HTAP","text":"<p>Antes de HTAP, los l\u00edmites eran claros:</p> <pre><code>\n  graph TD\n    A[Source Systems-OLTP ] --&gt;|Data| B[ Analytical Systems OLAP]</code></pre> <p>Aqu\u00ed podemos ver en la parte superior los Sistemas de registros; luego, los datos se mueven a trav\u00e9s de un proceso llamado Extraer, Transformar o Cargar (ETL) al sistema anal\u00edtico. (OLAP) Una clara distinci\u00f3n entre ambos tipos de sistemas. </p>"},{"location":"Season%201/S1E2/S1E2-Tipos%20de%20sistemas/#despues-de-htap","title":"Despu\u00e9s de HTAP","text":"<p>Un enfoque fue producir una base de datos que ofreciera toda la solidez y consistencia requerida por OLTP y la alta capacidad de procesamiento de datos requerida por OLAP en un solo producto. SAP prometi\u00f3 esto al lanzar SAP HANA. Otro enfoque fue tener una base de datos que almacenar\u00eda cada transacci\u00f3n simult\u00e1neamente en dos formatos, uno optimizado para OLAP y otro para OLTP. Un ejemplo de esto es: Azure Synapse Link para SQL. En el medio de estos dos enfoques aparece tambi\u00e9n la captura de datos (CDC). La idea es la misma que para Azure Synapse Link for SQL, pero es un poco m\u00e1s manual. El sistema OLTP transmite todas las transacciones que ha recibido y, por otro lado, podr\u00eda ser un sistema anal\u00edtico el que recibe estas transacciones (casi sin demora). Al final, tienes los datos almacenados en dos lugares.</p>"},{"location":"Season%201/S1E2/S1E2-TypeOfSystems/","title":"Type of Systems","text":"<p>During the years system evolved into two main categories (at least for the purpose of this blog): transactional and analytical / reporting systems.</p> <p>Transactional systems or systems of records are where the business events are recorded: A customer buys items. A patient has been prescribed medication. The oil of a machine is changed.</p> <p>The focus of these systems is usually consistency and concurrency. It won't be good for the business to lose or have incomplete data. And many transactions can or will happen simultaneously, meaning concurrency will be high. </p> <p>Analytical / reporting systems are where the data recorded by the systems of record is transferred to be analyzed. The focus of this system is a lot of storage (keeping history from multiple systems of records) and a high capacity for crunching data. Data can come from various systems, and the reports obtained from those systems can be complex.</p>"},{"location":"Season%201/S1E2/S1E2-TypeOfSystems/#transactional-systems-systems-of-records-oltp","title":"Transactional systems, Systems of records (OLTP)","text":"<p>OLTP stands for OnLine Transactional Processing. OLTP is a denomination also given to the database behind the systems of records. A database is a technology that stores data from a system or application.  You can use systems of records, transactional systems, or OLTP almost as synonyms. Remember that OLTP may be used more to indicate a database or technology than a system. Examples of Transactional systems are SAP S/4 HANA, Microsoft Dynamics, a bank system, a Point of Sale system, etc.</p>"},{"location":"Season%201/S1E2/S1E2-TypeOfSystems/#analytical-reporting-systems-olap","title":"Analytical / Reporting Systems (OLAP)","text":"<p>OLAP stands for OnLine Analytical Processing. OLAP is a denomination also given to the database behind Analytical / Reporting Systems. Examples of OLAP systems are Azure Synapse Dedicate Pools, Power BI datasets (Formerly Ms. Analysis Services), Snowflake, Google Cloud Big Query, AWS redshift, DuckDB, etc.</p>"},{"location":"Season%201/S1E2/S1E2-TypeOfSystems/#the-world-is-not-0s-or-1s","title":"The world is not 0s or 1s","text":"<p>Having finished high school and enrolled in university (Software Engineering at UPSA), I came to realize that life is more complicated than yes or no, 0s and 1s. Life happens in between.The same happened with OLTP and OLAP; in the beginning, it was a clear distinction, but guess what? Around 2014 Hybrid transaction/analytical processing (HTAP) appeared. It was a new approach, and its core concept was to avoid moving the data from the system of records to the analytical system, in other words: A single database that could serve both worlds, OLAP and OLTP. </p>"},{"location":"Season%201/S1E2/S1E2-TypeOfSystems/#before-htap","title":"Before HTAP","text":"<p>Before HTAP, boundaries were clear:</p> <p><pre><code>  graph TD\n    A[Source Systems-OLTP ] --&gt;|Data| B[ Analytical Systems OLAP]</code></pre> Here we can see on the top the Systems of records; then, data is moved via a process called Extract Transform or Load (ETL) to the Analytical system. (OLAP) A clear distinction between types of systems.</p>"},{"location":"Season%201/S1E2/S1E2-TypeOfSystems/#after-htap","title":"After HTAP","text":"<p>One approach was to produce a database that offered all the robustness and consistency required by OLTP and the high capacity for data crunching required by OLAP in one product. SAP promised this by releasing SAP HANA.Another approach was to have a database that would store each transaction simultaneously in two formats, one optimized for OLAP and one for OLTP. One example of this is: Azure Synapse Link for SQL. In the middle of these two approaches is changed data capture (CDC). The idea is the same as for Azure Synapse Link for SQL, but it is a bit more manual. The OLTP system broadcasts all transactions it has received, and on the other receiver side, it could be an analytical system that gets these transactions (with almost no delay). In the end, you have the data stored in two places.</p>"},{"location":"Season%201/S1E2/S1E2-TypeOfSystems/#summary-olap-vs-oltp","title":"Summary OLAP vs OLTP","text":"<p>This table provided by SAP wiki is my favourite:</p> <p>OLAP vs OLTP</p>"},{"location":"Season%201/S1E3/Modelando%20para%20OLTP/","title":"Modelo y dise\u00f1o OLTP","text":"<p>Sabemos por S1E2 que existen sistemas OLTP, OLAP y HTAP. Sabemos por S1E1 que podemos modelar los datos en dimensiones y tablas de hechos. Como habr\u00e1s adivinado, la forma de modelar los datos difiere si construyes un sistema OLAP u OLTP.</p> <p>El modelado de datos para OLTP suele ser as\u00ed:</p> <ul> <li> <p>Crear un modelo Entidad Relaci\u00f3n (ERD)</p> </li> <li> <p>Convierte ese ERD en tablas de base de datos</p> </li> <li> <p>Aplicar las formas normales</p> </li> </ul> <p>Todos estos conceptos son antiguos (de los a\u00f1os 70-80), pero siguen siendo v\u00e1lidos hoy en d\u00eda para las bases de datos relacionales.</p>"},{"location":"Season%201/S1E3/Modelando%20para%20OLTP/#el-modelo-entidad-relacion-erd","title":"El modelo Entidad-Relaci\u00f3n (ERD)","text":"<p>En 1976, Peter Chen public\u00f3 El modelo entidad-relaci\u00f3n-hacia una visi\u00f3n unificada de los datos. Puedes leer el documento original aqu\u00ed.</p> <p>A continuaci\u00f3n se muestra un ejemplo de un ERD:</p> <p></p> <p>Aqu\u00ed podemos ver dos tipos principales: Entidades: Cliente, Art\u00edculo Relaciones: compras</p> <p>Para las entidades y las relaciones, los atributos se representan como \u00f3valos. (ID, Nombre) (fecha, cantidad) (N\u00famero, descripci\u00f3n, precio) Para obtener una lista completa de los s\u00edmbolos que puedes utilizar en un ERD, consulta aqu\u00ed.</p> <p>Tambi\u00e9n podemos ver en el diagrama que el tipo de relaci\u00f3n es m:n, lo que significa que un n\u00famero m de clientes pueden comprar un n\u00famero n de art\u00edculos.</p> <p>Una vez dibujado el ERD, puedes convertirlo en tablas de base de datos relacional. Para las entidades, el proceso es bastante sencillo. Para cada entidad, habr\u00eda una tabla. La cosa se pone m\u00e1s entretenida para las relaciones: Como la relaci\u00f3n es m:n, la relaci\u00f3n se convertir\u00e1 en una nueva tabla. Todas las reglas se describen aqu\u00ed.</p>"},{"location":"Season%201/S1E3/Modelando%20para%20OLTP/#las-formas-normales","title":"Las formas normales","text":"<p>Por la misma \u00e9poca, Edgar Frank \"Ted\" Codd, considerado el padre de las bases de datos relacionales, public\u00f3 un art\u00edculo titulado \"Un modelo relacional de datos para grandes bancos de datos compartidos\".</p> <p>Unos a\u00f1os m\u00e1s tarde (1983), se present\u00f3 otro documento: Una gu\u00eda sencilla de cinco formas normales en la teor\u00eda de las bases de datos relacionales.</p> <p>Puedes leer todas las formas normales desde ese enlace; yo pondr\u00eda aqu\u00ed una descripci\u00f3n de la tercera forma normal (3NF):</p> <p>La tercera forma normal se viola cuando un campo no clave es un hecho sobre otro campo no clave, como en</p> <pre><code>| EMPLEADO | DEPARTAMENTO | UBICACI\u00d3N |\n|-------------|------------|----------|\n|(CAMPO CLAVE)|            |          | \n</code></pre> <p>El campo EMPLEADO es la clave. Si cada departamento est\u00e1 ubicado en un lugar, entonces el campo UBICACI\u00d3N es un dato sobre el DEPARTAMENTO, adem\u00e1s de ser un dato sobre el EMPLEADO. Los problemas de este dise\u00f1o son los mismos que los causados por las violaciones de la segunda forma normal.</p> <ul> <li>La ubicaci\u00f3n del departamento se repite en el registro de cada empleado asignado a ese departamento.</li> <li>Si cambia la ubicaci\u00f3n del departamento, hay que actualizar cada uno de esos registros.</li> <li>Debido a la redundancia, los datos pueden resultar incoherentes, por ejemplo, distintos registros que muestren distintas ubicaciones para el mismo departamento.</li> <li>Si un departamento no tiene empleados, puede que no haya ning\u00fan registro en cual guardar la ubicaci\u00f3n del departamento.</li> </ul> <p>Para satisfacer la tercera forma normal, el registro anterior debe descomponerse en dos registros:</p> <pre><code>| EMPLEADO     | DEPARTMENTO| \n|--------------|------------|\n|(CAMPO CLAVE) |            | \n\n\n| DEPARTMENTO  | UBICACI\u00d3N|\n|--------------|----------|\n|(CAMPO CLAVE) |          |\n</code></pre> <p>En resumen, un registro est\u00e1 en segunda y tercera formas normales si cada campo forma parte de la clave o proporciona un dato (de un solo valor) sobre exactamente toda la clave y nada m\u00e1s.</p> <p>Puedes utilizar el ERD y las formas normales para tener un dise\u00f1o limpio de una base de datos relacional (OLTP). En el pr\u00f3ximo episodio, examinaremos las t\u00e9cnicas para dise\u00f1ar bases de datos OLAP.</p>"},{"location":"Season%201/S1E3/OLTP%20modelling/","title":"OLTP model and design","text":"<p>We know from S1E2 that there are OLTP, OLAP, and HTAP systems. We know from S1E1 that we can model data into dimensions and fact tables. As you may have guessed, how to model the data differs if you build an OLAP or OLTP system.</p> <p>The data modeling for OLTP usually goes like this:</p> <p>\u2022 Create an Entity Relationship model. (ERD)</p> <p>\u2022 Convert that ERD into database tables</p> <p>\u2022 Apply the Normal forms</p> <p>All these concepts are ancient (from the '70 '80s) but are still valid today for relational databases.</p>"},{"location":"Season%201/S1E3/OLTP%20modelling/#the-entity-relationship-model-erd","title":"The Entity-relationship model (ERD)","text":"<p>In 1976, Peter Chen Published The entity-relationship model\u2014toward a unified view of data. You can read the original paper here.</p> <p>An example of an ERD follows:</p> <p></p> <p>Here we can see two main types: Entities: Customer, Item Relationships: buys</p> <p>For entities and relationship attributes are represented as ovals. (ID, Name) (date, quantity) (Number, description, price) For a complete list of symbols you can use in an ERD, see here.</p> <p>We can also see in the diagram that the relationship type is m:n, meaning an m number of customers can buy and n number of items.</p> <p>Once you have drawn the ERD, you can convert it to relational database tables. For the entities, the process is pretty simple. For each entity, there would be a table. It gets more enjoyable for the relationships: Since the relationship is m:n, the relationship will become a new table. All the rules are described here.</p>"},{"location":"Season%201/S1E3/OLTP%20modelling/#the-normal-forms","title":"The normal forms","text":"<p>Around the same time, Edgar Frank \"Ted\" Codd, considered the father of relational databases, published a paper called \"A Relational Model of Data for Large Shared Data Banks\".</p> <p>Some years later (1983), another paper was presented: A Simple Guide to Five Normal Forms in Relational Database Theory..</p> <p>You can read all the normal forms from that link; I would put here a description of the third normal form (3NF):</p> <p>Third normal form is violated when a nonkey field is a fact about another nonkey field, as in</p> <pre><code>| EMPLOYEE    | DEPARTMENT | LOCATION |\n|-------------|------------|----------|\n| (KEY FIELD) |            |          |\n</code></pre> <p>The EMPLOYEE field is the key. If each department is located in one place, then the LOCATION field is a fact about the DEPARTMENT.In addition to being a fact about the EMPLOYEE. The problems with this design are the same as those caused by violations of the second normal form. \u2022 The department's location is repeated in the record of every employee assigned to that department. \u2022 If the department's location changes, every such record must be updated. \u2022 Because of the redundancy, the data might become inconsistent, e.g., different records showing different locations for the same department. \u2022 If a department has no employees, there may be no record in which to keep the department's location.</p> <p>To satisfy third normal form, the record shown above should be decomposed into two records:</p> <pre><code>| EMPLOYEE    | DEPARTMENT | \n|-------------|------------|\n| (KEY FIELD) |            |\n\n| DEPARTMENT  | LOCATION |\n|-------------|----------|\n| (KEY FIELD) |          |\n</code></pre> <p>To summarize, a record is in second and third normal forms if every field is either part of the key or provides a (single-valued) fact about exactly the whole key and nothing else.</p> <p>You can use ERD and Normal forms to have a clean design for a relational database (OLTP). In the next episode, we will examine the techniques for designing OLAP databases.</p>"},{"location":"Season%201/S1E4/ModeladoOlap/","title":"S1E4 - Modelado OLAP","text":"<p>Los Datawarehouses son necesarios, para hacer consultas eficaces de los datos empesariales y suelen almacenar datos hist\u00f3ricos de diversas fuentes, como sistemas transaccionales, sistemas de apoyo a las operaciones y fuentes de datos externas. El concepto de Datawarehouse se introdujo por primera vez a finales de los a\u00f1os 80, cuando las empresas empezaron a darse cuenta del valor de analizar grandes cantidades de datos de distintas fuentes para tomar mejores decisiones empresariales. Desde entonces, el campo de Datawarehousing ha seguido evolucionando, con el desarrollo de nuevas tecnolog\u00edas y t\u00e9cnicas para mejorar el rendimiento y la escalabilidad de los almacenes de datos. Hoy en d\u00eda, los Datawarehouses son un componente fundamental de la infraestructura de BI de muchas organizaciones y sirven de apoyo a muchas funciones empresariales, como las finanzas, el marketing y las operaciones.</p> <p>En otras palabras:</p> <p>M\u00faltiples fuentes (sistemas de registros, OLTP) ---&gt;Datos---&gt; Almac\u00e9n de datos (t\u00edpicamente OLAP)</p> <p>Sabemos por S1E3 las t\u00e9cnicas que puedes utilizar para modelar OLTP (utilizando bases de datos relacionales), pero \u00bfqu\u00e9 pasa con OLAP o los almacenes de Datos? </p>"},{"location":"Season%201/S1E4/ModeladoOlap/#datawarehouses","title":"Datawarehouses","text":"<p>Una ilustraci\u00f3n simplificada de la \"Arquitectura\" de los data mart independientes. Estos silos anal\u00edticos independientes representan un Datawarehouse que, en esencia, no est\u00e1 arquitecturado. Aunque nadie presiona para que se aplique este enfoque, ocurre en todas partes. Las herramientas de BI de autoservicio sin una gobernanza adecuada son un catalizador de este enfoque. </p> <p></p>"},{"location":"Season%201/S1E4/ModeladoOlap/#inmon-fabrica-de-informacion-corporativa-hub-and-spoke","title":"Inmon (F\u00e1brica de Informaci\u00f3n Corporativa Hub-and-Spoke)","text":"<p>Aqu\u00ed los datos se extraen del sistema de registros y aterrizan en una base de datos 3NF(Tercera forma normal, ver temporadas anteriores) conocida como EDW (Almac\u00e9n de Datos Empresariales - Enterprise Data Warehouse). Los data marts pueden estructurarse dimensionalmente, pero difieren de la arquitectura Kimball porque est\u00e1n centrados en los departamentos y no en los procesos empresariales. Los usuarios consultar\u00e1n el EDW para obtener los datos m\u00e1s detallados; sin embargo, el ETL posterior tambi\u00e9n rellena los marts de datos.</p> <p></p>"},{"location":"Season%201/S1E4/ModeladoOlap/#kimball-arquitectura-datawarehouse","title":"Kimball (arquitectura Datawarehouse)","text":"<p>Los datos del \u00e1rea de presentaci\u00f3n deben ser dimensionales, at\u00f3micos (pueden tener agregados para mejorar el rendimiento), centrados en los procesos empresariales y adherirse a la arquitectura de bus del almac\u00e9n de datos de la empresa. NO debes estructurar los datos seg\u00fan la interpretaci\u00f3n que de ellos hagan los distintos departamentos.</p> <p></p>"},{"location":"Season%201/S1E4/ModeladoOlap/#hibrido-kimball-inmon","title":"H\u00edbrido (Kimball + Inmon)","text":"<p>Hay gente que afirma que es lo mejor de ambos mundos. </p> <p></p>"},{"location":"Season%201/S1E4/ModeladoOlap/#datavault","title":"Datavault","text":"<p>Te invito a que consultes este enlace donde encontrar\u00e1s la arquitectura de Datavault. Este diagrama tiene un sesgo de proveedor tecnol\u00f3gico, pero no he podido encontrar ning\u00fan otro de la Data Vault Alliance.</p>"},{"location":"Season%201/S1E4/OlapModelling/","title":"S1E4 - Modeling OLAP","text":"<p>Data warehouses are built to support the efficient querying and analysis of data and typically store historical data from various sources, such as transactional systems, operational systems, and external data sources. The concept of data warehousing was first introduced in the late 1980s, as companies began to realize the value of analyzing large amounts of data from different sources to make better business decisions. Since then, the field of data warehousing has continued to evolve, with new technologies and techniques being developed to improve the performance and scalability of data warehouses. Today, data warehouses are a critical component of many organizations' BI infrastructure and support many business functions, including finance, marketing, and operations.</p> <p>In other words:</p> <p>Multiple Sources (systems of records, OLTP) ---&gt;Data---&gt;   Datwarehouse (Typically OLAP)</p> <p>We know from S1E3 the techniques you can use for modeling OLTP (using relational databases), but what about OLAP or Data warehouses? </p>"},{"location":"Season%201/S1E4/OlapModelling/#datamarts","title":"DataMarts","text":"<p>A simplified illustration of the independent data mart \"Architecture.\" These standalone analytic silos represent a Datawarehouse that is essentially un-architected. Although nobody pushes to implement this approach, it happens everywhere. Self-service BI tools without proper governance are a catalysator for this approach. </p> <p></p>"},{"location":"Season%201/S1E4/OlapModelling/#inmon-hub-and-spoke-corporate-information-factory","title":"Inmon (Hub-and-Spoke Corporate Information Factory)","text":"<p>Here data is extracted from the system of records and lands in a 3NF(Third normal form, see previous seasons) database known as the EDW (Enterprise Data Warehouse). Datamarts can be dimensionally structured but differ from Kimball architecture because they are department centric rather than business process oriented. Users will query the EDW to get the most detailed data; however, subsequent ETL also populate data marts.</p> <p></p>"},{"location":"Season%201/S1E4/OlapModelling/#kimball-datawarehouse-architecture","title":"Kimball (Datawarehouse architecture)","text":"<p>Data in the presentation area must be dimensional, atomic (can have aggregates to improve performance), business process-centric, and adhere to the enterprise data warehouse bus architecture. You must NOT structure the data to individual departments' interpretation of the data.</p> <p></p>"},{"location":"Season%201/S1E4/OlapModelling/#hybrid-kimball-inmon","title":"Hybrid (Kimball + Inmon)","text":"<p>There are people that claim is the best of both worlds. </p> <p></p>"},{"location":"Season%201/S1E4/OlapModelling/#datavault","title":"Datavault","text":"<p>I invite you to check this link where you can find the architecture of Datavault. This diagram has a technology vendor bias, but could not find any other from the Data Vault Alliance.</p>"},{"location":"Season%201/S1E5/DeDwHastaLakeHouses/","title":"From Datawarehouses to Datalakes","text":"<p>En este art\u00edculo vamos a hablar de las diferencias entre los data warehouses, los data lakes y los data lakehouses, as\u00ed como a profundizar en algunas de sus ventajas e inconvenientes.</p>"},{"location":"Season%201/S1E5/DeDwHastaLakeHouses/#data-warehouses","title":"Data warehouses","text":"<p>En los a\u00f1os 80, los data warehouses eran la soluci\u00f3n que proporcionaba un modelo arquitect\u00f3nico para el flujo de datos desde los sistemas de registros, como los ERP, a los entornos de apoyo a la toma de decisiones, como SAP Business Warehouse, Oracle y otros.</p> <p>Las herramientas de BI y elaboraci\u00f3n de informes pod\u00edan conectarse a los data warehouses para generar cuadros de mando e informes, y tambi\u00e9n para apoyar a los responsables de la toma de decisiones.</p> <p>A medida que crec\u00edan los vol\u00famenes de datos y aumentaba su complejidad (los datos se volv\u00edan semiestructurados, anidados, etc.), los data warehouses se enfrentaban a algunos retos:</p> <p>Costes de mantenimiento elevados, principalmente costes de almacenamiento (almacenamiento propietario de alto rendimiento para un formato propietario).</p> <p>No hab\u00eda soporte para casos de aprendizaje autom\u00e1tico (machine learning), ya que s\u00f3lo estaba pensado para BI e informes.</p> <p>Falta de escalabilidad y flexibilidad para manejar diferentes complejidades de datos.</p> <p>Estos retos, la nube y otros factores empezaron a dar forma a algo nuevo: el data lake.</p>"},{"location":"Season%201/S1E5/DeDwHastaLakeHouses/#data-lakes","title":"Data lakes","text":"<p>En los primeros d\u00edas del movimiento de los data lakes, Hadoop era el componente principal. Hubo historias de \u00e9xito, hubo historias de fracaso, sin embargo, hab\u00eda comenzado una nueva era:</p> <p>En muchos casos, las empresas pudieron sustituir el costoso data warehouse por cl\u00fasteres inform\u00e1ticos internos que ejecutaban Hadoop de c\u00f3digo abierto.</p> <p>Esto permiti\u00f3 a las empresas analizar cantidades masivas de datos no estructurados (tambi\u00e9n llamados big data) de una forma que antes no era posible.</p> <p>Hoy en d\u00eda parece que Spark es quien lleva la batuta como motor de los data lakes. Pero, \u00bfcu\u00e1les son los nuevos retos que plantean los data lakes?</p> <p>Carecen de algunas caracter\u00edsticas b\u00e1sicas que estuvieron disponibles durante d\u00e9cadas en los RDBMS y data warehouses, tales como</p> <p>soporte para transacciones</p> <p>aplicaci\u00f3n de la calidad de los datos (como tipos de datos formales)</p> <p>soporte de anexos y actualizaciones (insert, update) sin tener que volver a procesar varios archivos</p> <p>soporte de bloqueo (o mecanismo similar) para evitar la incoherencia de las actualizaciones procedentes de lotes o flujos.</p> <p>Pero, por otro lado, los data lakes ofrecen un almacenamiento extremadamente asequible y duradero, como Azure Storage o AWS S3, y son facilitadores de casos de uso de aprendizaje autom\u00e1tico.</p> <p>Por eso, las empresas acaban teniendo una mezcla de tecnolog\u00edas: data lakes, data warehouses, soluciones de streaming y otras. Pero disponer de m\u00faltiples tecnolog\u00edas aumenta la complejidad, incrementa los costes y crea la necesidad de copiar los datos a m\u00faltiples ubicaciones, lo que a su vez aumenta la latencia y de nuevo los costes, y desde el punto de vista de la seguridad tambi\u00e9n aumenta la superficie de ataque.</p>"},{"location":"Season%201/S1E5/DeDwHastaLakeHouses/#lakehouses","title":"Lakehouses","text":"<p>La promesa de Lakehouse es combinar las mejores caracter\u00edsticas tanto de los almacenes de datos como de los lagos de datos. Intentan habilitar las funciones de gesti\u00f3n de datos de los almacenes de datos directamente en el almacenamiento de bajo coste (por ejemplo, Azure Storage, AWS S3) que utilizan los lagos de datos.</p> <p>Ahora me pondr\u00e9 un poco m\u00e1s t\u00e9cnico y m\u00e1s detallado, porque \u00e9sta es la parte interesante.</p> <p>Imagina que eres un minorista y tienes una tabla llamada Art\u00edculos. Esta tabla tiene 9 millones de registros. \u00bfC\u00f3mo la almacenar\u00edas en un lago de datos? Un enfoque com\u00fanmente utilizado es con archivos de parquet:</p> <p>Tal vez 18 archivos que contengan aprox. 0,5 MM de registros cada uno. En el mejor de los casos, para esta \u00fanica tabla tendr\u00e1s que manejar unos 20 archivos de este tipo. Incluso podr\u00eda haber 1.000 archivos representando la tabla Art\u00edculos en una implementaci\u00f3n sub\u00f3ptima. La cuesti\u00f3n es que en un lago de datos acabar\u00e1s encontr\u00e1ndote con el problema de \"demasiados archivos\", tarde o temprano. Recuerda que en la vida real tendr\u00e1s m\u00e1s entidades adem\u00e1s de los art\u00edculos, lo que contribuir\u00e1 a tener m\u00e1s archivos.</p> <p>Ahora imagina que necesitas actualizar 10.000 art\u00edculos (por ejemplo, para a\u00f1adir un 1% a su precio). Tendr\u00e1s que encontrar en qu\u00e9 archivos se encuentran esos productos, luego borrar los archivos originales y escribir otros nuevos en su lugar. Mientras esto ocurre, algunos procesos podr\u00edan estar intentando leer esos archivos, pero fallar\u00e1n. En otras palabras, modificar los datos existentes es muy costoso, complejo y poco fiable.</p> <p>En alg\u00fan momento querr\u00e1s mirar atr\u00e1s y determinar cu\u00e1ndo se modific\u00f3 el precio y a qu\u00e9 art\u00edculos. En el mundo de los lagos de datos, para mantener un historial de los cambios, tendr\u00e1s que crear y mantener copias de los archivos originales, lo que genera una sobrecarga importante.</p> <p>Tambi\u00e9n es posible que quieras un flujo de datos en streaming para actualizar los art\u00edculos, pero al mismo tiempo hay un proceso por lotes realizando una actualizaci\u00f3n. Y como no existe ning\u00fan mecanismo de bloqueo, esto no es factible.</p> <p>Como la inform\u00e1tica que responde a las consultas va \"a ciegas\" (sin pistas como los \u00edndices) y escanea varios o todos los archivos, el rendimiento no es muy bueno en los lagos de datos.</p> <p>Por \u00faltimo, todos estos puntos problem\u00e1ticos contribuyen a tener problemas de mala calidad de los datos.</p> <p>Las respuestas de Lakehouse a los retos anteriores son</p> <p>Transacciones ACID: cada operaci\u00f3n es transaccional. Esto significa que cada operaci\u00f3n tiene \u00e9xito o se cancela. Cuando se aborta, se registra, y cualquier residuo se limpia para que puedas volver a intentarlo m\u00e1s tarde. La modificaci\u00f3n de los datos existentes es posible porque las transacciones te permiten realizar actualizaciones detalladas. Las operaciones en tiempo real son coherentes, y las versiones hist\u00f3ricas de los datos se almacenan autom\u00e1ticamente. Tambi\u00e9n proporciona instant\u00e1neas de los datos para que los desarrolladores puedan acceder f\u00e1cilmente a versiones anteriores y volver a ellas para auditor\u00edas, reversiones o reproducciones de experimentos.</p> <p>Indexaci\u00f3n: mecanismo que ayuda al ordenador a leer menos archivos para encontrar la informaci\u00f3n necesaria.</p> <p>Validaci\u00f3n de esquemas: los datos que introduces en la base de datos deben adherirse a un esquema definido. Si los datos no se adhieren, se pueden mover a una cuarentena y se env\u00eda una notificaci\u00f3n automatizada.</p> <p>Pero, \u00bfc\u00f3mo proporcionar estas funciones sobre un almacenamiento barato como el de los lagos de datos?</p> <p>Con formatos de archivo y marcos como</p> <p>Delta Lake</p> <p>Apache Iceberg</p> <p>Apache Hudi</p> <p>etc.</p> <p>Combinas cualquiera de estos formatos con un motor de c\u00e1lculo como Spark y obtienes transacciones ACID, indexaci\u00f3n, versionado, etc. sobre archivos gestionados autom\u00e1ticamente y almacenados en un almacenamiento barato del lago de datos.</p> <p>Volviendo a nuestra tabla Art\u00edculos, si por ejemplo eliges Delta Lake, obtendr\u00e1s una carpeta X que contiene los archivos con datos de Art\u00edculos. No tendr\u00e1s que preocuparte de cu\u00e1ntos archivos ni nada parecido.</p> <p>Tambi\u00e9n puedes tener un cat\u00e1logo que har\u00e1 coincidir la tabla Art\u00edculos y el contenido de la carpeta X. Gracias a esto, puedes utilizar SQL, o tu lenguaje preferido, para consultar, actualizar, insertar o eliminar registros sin conocer los archivos concretos. Ahora tienes una tabla real (como en un RDBMS), pero el almacenamiento est\u00e1 en un lago de datos barato y duradero.</p> <p>Hay otras ofertas SaaS como Snowflake, BigQuery, etc. que proporcionan las mismas o incluso m\u00e1s prestaciones con precios de almacenamiento tan bajos como los que ofrece Datalakes. Pero si utilizas un formato abierto como Delta Lake, Iceberg, etc. puedes elegir el motor que mejor se adapte a tus necesidades.</p>"},{"location":"Season%201/S1E5/FromDataWarehousesToDataLakes/","title":"From Datawarehouses to Datalakes","text":"<p>In this article we're going to discuss the differences between data warehouses, data lakes and lakehouses, as well as take a deeper look at some of their benefits and drawbacks.</p>"},{"location":"Season%201/S1E5/FromDataWarehousesToDataLakes/#data-warehouses","title":"Data warehouses","text":"<p>Back in the 80s, data warehouses were the solution that provided an architectural model for the flow of data from systems of records like ERPs to decision support environments like SAP Business Warehouse, Oracle, and others.</p> <p>BI and reporting tools where able to connect to the data warehouses to generate dashboards and reports, and also to support decision makers.</p> <p>As data volumes grew and the complexity of the data increased (data became semi-structured, nested, etc.), data warehouses had some challenges:</p> <p>High maintenance costs, mainly storage costs (high performance proprietary storage for a proprietary format).</p> <p>There was no support for machine learning cases, as it was only meant for BI and reporting.</p> <p>Lack of scalability and flexibility for handling different data complexities.</p> <p>These challenges, the cloud and other factors started to shape into something new: the data lake.</p>"},{"location":"Season%201/S1E5/FromDataWarehousesToDataLakes/#data-lakes","title":"Data lakes","text":"<p>In the earlier days of the data lake movement, Hadoop was the main component. There were success stories, there were failure stories, nevertheless a new era had begun:</p> <p>In many cases, companies have been able to replace expensive data warehouse software with in-house computing clusters running open source Hadoop.</p> <p>It allowed companies to analyse massive amounts of unstructured data (also called big data) in a way that wasn\u2019t possible before.</p> <p>Nowadays it seems that Spark is the one running the show as the engine for data lakes. But what are the new challenges the data lakes yield?</p> <p>They lack some basic features that were available for decades in RDBMS and data warehouses such as:</p> <p>support for transactions</p> <p>enforcement of data quality (like formal data types)</p> <p>support of appends and updates without having to re-process multiple files</p> <p>support of locking (or similar mechanism) to avoid inconsistency of updates coming from batch or stream</p> <p>But on the other hand data lakes offer an extremely affordable and durable storage like Azure Storage or AWS S3, and are enablers of machine learning use cases.</p> <p>Because of this, companies ended up having a mix of technologies: data lakes, data warehouses, streaming solutions, and others. But having multiple technologies increases the complexity, increases costs, and creates the need to copy data to multiple locations, which in turn increases latency and again costs, and from a security perspective it also increases the attack surface.</p>"},{"location":"Season%201/S1E5/FromDataWarehousesToDataLakes/#lakehouses","title":"Lakehouses","text":"<p>Lakehouse\u2019s promise is to combine the best features of both data warehouses and data lakes. They are trying to enable data management features of data warehouses directly on low-cost storage (e.g Azure Storage, AWS S3) used by delta lakes.</p> <p>Now I will get a bit more technical and more detailed, because this is the interesting part.</p> <p>Imagine you are a retailer, you have a table called Items. This table has 9 million records. How would you store it in a data lake? One commonly used approach is with parquet files:</p> <p>Maybe 18 files holding aprox. 0.5 MM records each. In a best case scenario, for this one table you\u2019ll have to handle around 20 such files. There could even be 1,000 files representing the Items table in a suboptimal implementation. The point is that in a data lake you will end up with the \u201ctoo many files\u201d problem, sooner or later. Remember that in real life you will have more entities than just the items, which will contribute to having more files.</p> <p>Now imagine you need to update 10,000 items (for example to add 1% to their price). You will need to find in which files those products are located, then delete the original files and write new ones instead. While this is happening, some processes could be trying to read those files but will fail. In other words, modifying existing data is very costly, complex and unreliable.</p> <p>At some point you want to look back and determine when the price was changed and to which articles. In the data lake world, to keep a history of changes, you will need to create and maintain copies of the original files, which creates a significant overhead.</p> <p>You may also want a streaming data flow to update the items, but at the same time there is a batch processes doing an update. And since there are no mechanism for locking, this is not feasible.</p> <p>Since the computing that respond to queries goes \u201cblindly\u201d (without hints like indexes) and scan multiple or all the files, performance is not great in data lakes.</p> <p>Lastly, all these pain points contribute to having poor data quality issues.</p> <p>Lakehouse answers to the challenges above are:</p> <p>ACID transactions: every operation is transactional. This means that every operation either fully succeeds or is aborted. When aborted, it is logged, and any residue is cleaned so you can retry later. Modification of existing data is possible because transactions allow you to do fine-grained updates. Real-time operations are consistent, and the historical data versions are automatically stored. The lakehouse also provides snapshots of data to allow developers to easily access and revert to earlier versions for audits, rollbacks, or experiment reproductions.</p> <p>Indexing: a mechanism that helps the compute to read less files to find the needed information.</p> <p>Schema validation: the data you put in the lakehouse must adhere to a defined schema. If data doesn\u2019t adhere it can be moved to a quarantine and an automated notification sent out.</p> <p>But how do you provide these features on top of cheap storage like the one of data lakes?</p> <p>With file formats and frameworks like:</p> <p>Delta Lake</p> <p>Apache Iceberg</p> <p>Apache Hudi</p> <p>Etc.</p> <p>You combine any of these formats with a compute engine like Spark and you get ACID transactions, indexing, versioning, etc. on top of automatically managed files stored in cheap storage from the data lake.</p> <p>Going back to our Items table, if for example you choose Delta Lake, you will get a folder X containing the files with Items data. No need to worry about how many files, or nothing like that.</p> <p>You can also have a catalog that will match table Items and the contents of folder X. Thanks to this, you can use SQL, or your preferred language, to query, update, insert or delete records without knowing the specific files. Now you have an actual table (like in RDBMS) but the storage is in a cheap and durable data lake.</p> <p>There are other SaaS offerings like Snowflake, BigQuery, Etc. that provide same or even more features with storages prices as low as the ones offered by Datalakes. But if you use an open format like Delta Lake, Iceberg, etc. you can choose the engine that suits your needs bests.</p> <p>This article is part I, in part II I will dig deeper in the different engines and formats.</p>"},{"location":"Season%202/S2E1/S2E1%20Keys/","title":"Keys","text":"<p>\"The key, the whole key, and nothing but the key, so help me, Codd.\"</p> <p>In S1E3, we introduced Edgar Frank \"Ted\" Codd, Considered the founder of relational databases. And that is a famous quote related to primary keys. </p> <p>A primary key in OLTP and relational databases is vital, and all relational databases enforce a primary key.</p> <p>As Bill Inmon and Francesco Puppin explain in their book the Unified star schema, The term \"Primary Key\" may generate a bit of confusion because, in the world of relational databases, it may have a specific meaning of \"enforced Primary Key\": a mechanism that produces an error when the uniqueness of the key is violated.</p> <p>If we check the documentation of products like Snowflake or Databricks, we see statements like this:</p> <p>\"Snowflake supports defining and maintaining constraints, but does not enforce them, except for NOT NULL constraints, which are always enforced.\"</p> <p>https://docs.snowflake.com/en/sql-reference/constraints-overview</p> <p>\"Informational primary key and foreign key constraints encode relationships between fields in tables and are not enforced.\" https://docs.databricks.com/tables/constraints.html</p> <p>In both products (and many others), we see that they do not enforce the primary key, but that responsibility is left to the processes that write the data.</p> <p>This difference in how data warehouse engines handle the primary key suggests that we may need a different approach for managing the keys in data warehouses and datamarts (mainly OLAP).</p>"},{"location":"Season%202/S2E1/S2E1%20Keys/#natural-key-business-key","title":"Natural Key / Business Key","text":"<p>Patrick Cuba, in his book The Data Vault Guru, defines:</p> <p>What are natural keys? How does the business identify an individual entity, be it a customer, product, account, contract, workshop, or factory? These are unique values that are used within the enterprise and sometimes externally to the enterprise in discussions with customers, service providers, partners, etc. They identify the thing we want to transact on or with, uniquely track the things that make a business viable, may need regulatory reporting on, and are used to uniquely identify a person, company, account, product, or thing through its value to the business. To the subject area of concern, these identifying values have meaning, and data vault tracks everything around those keys by historizing data about these things and providing the audit trail on everything we know about the thing. In the data vault, natural keys are our first-choice business key candidates; in Kimball terminology, they are called natural keys because, for the life of a business entity likely, we can uniquely identify that entity by that key. The business entity may be represented by different keys depending on the source system or platform as long as it holds the same semantic grain. Other meaningful keys include things like vehicle identification numbers, flight numbers, purchase order number, barcodes, SKUs; you see these are the uniquely identifying keys you will likely use when interacting with for doing business, for commerce. They are the first-choice business keys in data vault. A test for identifying a business key is this: Can the key value with no other attribute uniquely identify a thing? \u2022 Does the key with no other attribute translate to an area of importance within an enterprise? When tracking information about a customer or account, what is the unique identification number that the thing is tied to? We use natural keys every day, the account number we use to top-up our public transport card, the account number where details of our utility bills are tracked, the bank account number where our disposable funds are kept. Some keys are shared and some are uniquely assigned to a customer. Some natural keys are highly confidential, and some include embedded logic that tells us more about that key.</p>"},{"location":"Season%202/S2E1/S2E1%20Keys/#hash-keys","title":"Hash keys","text":"<p>Hash keys are durable surrogate keys that aim to optimize loading and data distribution but are not strictly for optimizing reporting.</p> <p>We are not dealing with transactions in Data warehouses as in OLTP systems or systems of records. Suppose a customer changes the quantity of an order in an OLTP system. In that case, an update in a specific record (that order from that customer needs to be updated using a primary key that identifies the order) needs to happen.</p> <p>From the data warehouse point of view, we will receive a newer version of the order from the source system. Usually, data warehouses store snapshots of the data from the source system. We can see that the primary key constraint is no longer critical; it is ok to have two orders with the same business key in the data warehouse; depending on your methodology, there are different methods.</p> <p>The most intuitive method is to sort by the last ingestion date and find the first record for each business key. For example, there will be two records with the same business key (order number) in the orders, and we will pick the one with the last ingestion date.</p> <p>Depending on the modeling technique and business requirements, more advanced techniques like the data vault's PIT (Point in time table) exist.</p> <p>In other words, read your methodology documentation (Kimball, Data Vault, etc.)  to find the recommended practices for dealing with keys in OLAP. (Data warehouses, data marts, etc.)</p> <p>Since you may be dealing with multiple source systems or source systems will be replaced, the most common approaches to dealing with keys include these steps</p> <p>Prepare the business key Leading or trailing zeros, case (convert all to uppercase), spaces, etc. Multi-column business keys can be concatenated into one Apply a hashing or a sequence to create a new key.</p> <p>Performance is also something to keep in mind; the data type you choose for your keys (that will be used for doing the joins) does matter.</p> <p>Found this interesting blog about data types and performance in joins for Delta Lake format: https://www.confessionsofadataguy.com/data-types-in-delta-lake-spark-join-performance-and-thoughts/</p>"},{"location":"Season%202/S2E1/S2E1%20Llaves/","title":"LLaves o Claves","text":"<p>\"The key, the whole key, and nothing but the key, so help me, Codd.\"</p> <p>En la S1E3, presentamos a Edgar Frank \"Ted\" Codd, considerado el fundador de las bases de datos relacionales. Y esta es una cita famosa relacionada con las claves primarias.</p> <p>Una clave primaria en las bases de datos OLTP y relacionales es vital, y todas las bases de datos relacionales soportan una clave primaria.</p> <p>Como explican Bill Inmon y Francesco Puppin en su libro El esquema estrella unificado, El t\u00e9rmino \"clave primaria\" puede generar un poco de confusi\u00f3n porque, en el mundo de las bases de datos relacionales, puede tener un significado espec\u00edfico de \"clave primaria forzada\": un mecanismo que produce un error cuando se viola la unicidad de la clave.</p> <p>Si consultamos la documentaci\u00f3n de productos como Snowflake o Databricks, veremos afirmaciones como \u00e9sta</p> <p>\"Snowflake permite definir y mantener restricciones, pero no las impone, excepto las restricciones NOT NULL, que siempre se imponen\".</p> <p>https://docs.snowflake.com/en/sql-reference/constraints-overview</p> <p>\"Las restricciones informativas de clave primaria y clave externa codifican las relaciones entre los campos de las tablas y no se aplican\". https://docs.databricks.com/tables/constraints.html</p> <p>En ambos productos (y en muchos otros), vemos que no aplican la clave primaria, sino que esa responsabilidad se deja a los procesos que escriben los datos.</p> <p>Esta diferencia en la forma en que los motores de almac\u00e9n de datos gestionan la clave primaria sugiere que quiz\u00e1 necesitemos un enfoque diferente para gestionar las claves en los almacenes de datos y los datamarts (principalmente OLAP).</p>"},{"location":"Season%202/S2E1/S2E1%20Llaves/#clave-natural-clave-comercial-natural-key-business-key","title":"Clave natural / Clave comercial (Natural Key / Business Key)","text":"<p>Patrick Cuba, en su libro El Gur\u00fa de la B\u00f3veda de Datos (Data Vault) , define:</p> <p>\u00bfQu\u00e9 son las claves naturales? \u00bfC\u00f3mo identifica la empresa a una entidad individual, ya sea un cliente, un producto, una cuenta, un contrato, un taller o una f\u00e1brica? Son valores \u00fanicos que se utilizan dentro de la empresa y, a veces, fuera de ella, en las conversaciones con clientes, proveedores de servicios, socios, etc. Identifican aquello sobre lo que queremos realizar transacciones o con lo que queremos realizar transacciones, rastrean de forma \u00fanica las cosas que hacen viable un negocio, sobre las que puede ser necesario elaborar informes reglamentarios, y se utilizan para identificar de forma \u00fanica a una persona, empresa, cuenta, producto o cosa por su valor para la empresa.</p> <p>Para el tema que nos ocupa, estos valores identificativos tienen un significado, y la b\u00f3veda de datos rastrea todo lo que rodea a esas claves historizando los datos sobre esas cosas y proporcionando la pista de auditor\u00eda sobre todo lo que sabemos de la cosa.</p> <p>En la b\u00f3veda de datos, las claves naturales son nuestras candidatas a claves empresariales de primera elecci\u00f3n; en la terminolog\u00eda de Kimball, se denominan claves naturales porque, durante la vida probable de una entidad empresarial, podemos identificar de forma \u00fanica a esa entidad mediante esa clave. La entidad empresarial puede estar representada por claves diferentes seg\u00fan el sistema o plataforma de origen, siempre que mantenga el mismo grano sem\u00e1ntico.</p> <p>Otras claves significativas incluyen cosas como n\u00fameros de identificaci\u00f3n de veh\u00edculos, n\u00fameros de vuelo, n\u00famero de orden de compra, c\u00f3digos de barras, SKU; ver\u00e1s que \u00e9stas son las claves de identificaci\u00f3n \u00fanica que probablemente utilizar\u00e1s cuando interact\u00faes para hacer negocio. Son las claves empresariales de primera elecci\u00f3n en la b\u00f3veda de datos.</p> <p>Una prueba para identificar una clave comercial es \u00e9sta: \u00bfPuede el valor de la clave, sin ning\u00fan otro atributo, identificar una cosa de forma \u00fanica?</p> <ul> <li>\u00bfLa clave sin otro atributo se traduce en un \u00e1rea de importancia dentro de una empresa? Al rastrear informaci\u00f3n sobre un cliente o una cuenta, \u00bfcu\u00e1l es el n\u00famero de identificaci\u00f3n \u00fanico al que est\u00e1 vinculada la cosa? Utilizamos claves naturales todos los d\u00edas, el n\u00famero de cuenta que utilizamos para recargar nuestra tarjeta de transporte p\u00fablico, el n\u00famero de cuenta donde se rastrean los detalles de nuestras facturas de servicios p\u00fablicos, el n\u00famero de cuenta bancaria donde se guardan nuestros fondos disponibles. Algunas claves se comparten y otras se asignan exclusivamente a un cliente. Algunas claves naturales son altamente confidenciales, y otras incluyen una l\u00f3gica incorporada que nos dice m\u00e1s cosas sobre esa clave.</li> </ul>"},{"location":"Season%202/S2E1/S2E1%20Llaves/#claves-hash","title":"Claves hash","text":"<p>Las claves hash son claves sustitutas duraderas que pretenden optimizar la carga y la distribuci\u00f3n de datos, pero no sirven estrictamente para optimizar la elaboraci\u00f3n de informes.</p> <p>En los almacenes de datos (data warehouses) no se trata de transacciones como en los sistemas OLTP o sistemas de registros. Supongamos que un cliente modifica la cantidad de un pedido en un sistema OLTP. En ese caso, debe producirse una actualizaci\u00f3n en un registro concreto (el pedido de ese cliente debe actualizarse utilizando una clave primaria que identifique el pedido).</p> <p>Desde el punto de vista del almac\u00e9n de datos, recibiremos una versi\u00f3n m\u00e1s reciente del pedido desde el sistema fuente. Normalmente, los almacenes de datos guardan instant\u00e1neas de los datos del sistema fuente. Podemos ver que la restricci\u00f3n de clave primaria ya no es cr\u00edtica; no pasa nada por tener dos pedidos con la misma clave de negocio en el almac\u00e9n de datos; dependiendo de tu metodolog\u00eda, existen distintos m\u00e9todos.</p> <p>El m\u00e9todo m\u00e1s intuitivo es ordenar por la \u00faltima fecha de ingesta y encontrar el primer registro para cada clave de negocio. Por ejemplo, habr\u00e1 dos registros con la misma clave de negocio (n\u00famero de pedido) en los pedidos, y elegiremos el que tenga la \u00faltima fecha de ingesta.</p> <p>Dependiendo de la t\u00e9cnica de modelado y de los requisitos empresariales, existen t\u00e9cnicas m\u00e1s avanzadas, como la tabla PIT (Point in time table) de la b\u00f3veda de datos.</p> <p>En otras palabras, lee la documentaci\u00f3n de tu metodolog\u00eda (Kimball, B\u00f3veda de datos, etc.) para encontrar las pr\u00e1cticas recomendadas para tratar las claves en OLAP. (Almacenes de datos, data marts, etc.)</p> <p>Dado que puedes estar tratando con m\u00faltiples sistemas fuente o que los sistemas fuente ser\u00e1n sustituidos, los enfoques m\u00e1s comunes para tratar las claves incluyen estos pasos</p> <p>Prepara la clave de negocio Ceros iniciales o finales, may\u00fasculas y min\u00fasculas (convertir todo a may\u00fasculas), espacios, etc. Las claves empresariales de varias columnas pueden concatenarse en una sola Aplica un hashing o una secuencia para crear una nueva clave.</p> <p>El rendimiento tambi\u00e9n es algo a tener en cuenta; el tipo de datos que elijas para tus claves (que se utilizar\u00e1n para hacer las uniones) s\u00ed importa.</p> <p>He encontrado este interesante blog sobre tipos de datos y rendimiento en las uniones para el formato Delta Lake: https://www.confessionsofadataguy.com/data-types-in-delta-lake-spark-join-performance-and-thoughts/</p>"},{"location":"Season%202/S2E2/UsandoOpenAIparaIngDeDatos/","title":"OpenAI e Ingenier\u00eda de datos","text":"<p>El primer episodio en video, dale un mirada aqu\u00ed: https://www.youtube.com/watch?v=iaQUw7cKsuk</p> <p>Descubre como usar OpenAI + Synapse para limpiar tus datos en tus pipelines. El c\u00f3digo fuente esta disponible aqu\u00ed: https://github.com/thcosters/SynapseSparkGPT/tree/main/DataCleansing</p> <p>Inspirado en: https://www.youtube.com/watch?v=T5zxrQDCS1g&amp;t=0s</p>"},{"location":"Season%202/S2E3/UnifiedStarSchema/","title":"The unified star schema","text":"<p>As we saw in S1E4, Ralf Kimball published almost 30 years ago the foundation of dimensional modeling.</p> <p>Then Francesco Puppini joined forces, and in 2020 they published the Unified Star Schema (USS). Get the book if you work with dimensional modeling. Even if you do not take the road of USS, there is a lot of helpful material in the book, like Fan and Chasm Traps, etc.</p> <p>I recommend you watch this video to get an idea of the way of the USS.</p> <p>If you want to explore the USS, here is a simple SQL script that creates the \"bridge\" table as a view in Snowflake. You only need a free Snowflake account and run the script.</p> <p>I also welcome improvements to the bridge table (to include other tables, etc.). The idea is to have a bridge table for people to try the USS in a well know model like TPCH.</p>"},{"location":"Season%203/S3E1/AZD_EasyAuthdemo/","title":"Deploying a Web Application to Azure","text":"<p>Deploying a data intensive web application to Azure. Leveraging Code Spaces, AZD and Azure Easy Auth.</p>"},{"location":"Season%203/S3E1/AZD_EasyAuthdemo/#video-part-i-deploying-a-streamlit-application-to-azure-with-two-commands","title":"Video Part I - Deploying a Streamlit Application to Azure with two commands","text":"<p>View Part I</p>"},{"location":"Season%203/S3E1/AZD_EasyAuthdemo/#video-part-ii-setting-continous-deployment-cd-pipeline-with-github-actions","title":"Video Part II - Setting Continous deployment (CD) pipeline with Github actions","text":"<p>View part II</p>"},{"location":"Season%203/S3E1/AZD_EasyAuthdemo/#link-to-the-repo-shown-in-the-videos","title":"Link to the REPO shown in the videos","text":"<p>See the repo here</p>"}]}